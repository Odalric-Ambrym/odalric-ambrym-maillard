{"id":39,"date":"2016-03-12T11:49:17","date_gmt":"2016-03-12T10:49:17","guid":{"rendered":"http:\/\/odalricambrymmaillard.neowordpress.fr\/?page_id=39"},"modified":"2016-03-12T12:40:33","modified_gmt":"2016-03-12T11:40:33","slug":"reinforcement-learning","status":"publish","type":"page","link":"http:\/\/odalricambrymmaillard.neowordpress.fr\/research-areas\/reinforcement-learning\/","title":{"rendered":"Reinforcement Learning"},"content":{"rendered":"<p><strong><a title=\"\u201cHow hard is my MDP?\u201d Distribution-norm to the rescue.\" href=\"http:\/\/odalricambrymmaillard.wordpress.com\/2014\/12\/09\/how-hard-is-my-mdp-distribution-norm-to-the-rescue\/\">\u201cHow hard is my MDP?\u201d Distribution-norm to the rescue<\/a>, <\/strong>Odalric-Ambrym Maillard, Timothy A. Mann and Shie Mannor <em>in Proceedings of the 27th conference on advances in Neural Information Processing Systems (NIPS), 2014. <a href=\"http:\/\/papers.nips.cc\/paper\/5441-how-hard-is-my-mdp-the-distribution-norm-to-the-rescue\">Publisher website<\/a> HaL<\/em><\/p>\n<p><strong><a href=\"http:\/\/odalricambrymmaillard.wordpress.com\/2014\/11\/13\/selecting-near-optimal-approximate-state-representations-in-reinforcement-learning\/\">Selecting Near-Optimal Approximate State Representations in Reinforcement Learning<\/a>, <\/strong>R.Ortner, O.-A. Maillard and D. Ryabko, <em>in Proceedings of the International Conference on Algorithmic Learning Theory (ALT), 2014. Publisher website <a href=\"https:\/\/hal.inria.fr\/hal-01057562\">HaL<\/a><\/em><\/p>\n<p><a href=\"http:\/\/odalricambrymmaillard.wordpress.com\/2014\/08\/09\/competing-with-an-infinite-set-of-models-in-reinforcement-learning\/\"><strong>Competing with an infinite set of models in reinforcement learning<\/strong><\/a>, P. Nguyen, O.-A. Maillard, D. Ryabko, and R. Ortner, <em>in Proceedings of the International Conference on Artificial Intelligence and Statistics (AI&amp;STATS), volume 31 of JMLR W&amp;CP , pages 463\u2013471, Arizona, USA, 2013. <a href=\"http:\/\/jmlr.org\/proceedings\/papers\/v31\/nguyen13a.pdf\">Publisher website<\/a> HaL<\/em><\/p>\n<p><a href=\"http:\/\/odalricambrymmaillard.wordpress.com\/2014\/08\/08\/optimal-regret-bounds-for-selecting-the-state-representation-in-reinforcement-learning\/\"><strong>Optimal regret bounds for selecting the state representation in reinforcement learning<\/strong><\/a>, O.-A. Maillard, P. Nguyen, R. Ortner, and D. Ryabko, <em>in Proceedings of the International conference on Machine Learning (ICML), volume 28 of JMLR W&amp;CP, pages 543\u2013551, Atlanta, USA, 2013. <a href=\"http:\/\/jmlr.org\/proceedings\/papers\/v28\/maillard13.pdf\">Publisher website<\/a> <a href=\"http:\/\/hal.archives-ouvertes.fr\/hal-00778586\/\">HaL<\/a><\/em><\/p>\n<p><a href=\"http:\/\/odalricambrymmaillard.wordpress.com\/2014\/08\/08\/hierarchical-optimistic-region-selection-driven-by-curiosity\/\"><strong>Hierarchical optimistic region selection driven by curiosity<\/strong><\/a>, O.-A. Maillard, in P. Bartlett, F.C.N. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, editors, Proceedings of the conference on advances in Neural Information Processing Systems 25 (NIPS), pages 1457\u20131465, 2012. <em><a href=\"http:\/\/papers.nips.cc\/paper\/4580-hierarchical-optimistic-region-selection-driven-by-curiosity\">Publisher website<\/a> <a href=\"http:\/\/hal.archives-ouvertes.fr\/hal-00740418\/\">HaL<\/a><\/em><\/p>\n<p><a href=\"http:\/\/odalricambrymmaillard.wordpress.com\/2014\/08\/05\/selecting-the-state-representation-in-reinforcement-learning\/\"><strong>Selecting the state-representation in reinforcement learning<\/strong><\/a> O.-A. Maillard, D. Ryabko, and R. Munos, in Proceedings of the 24th conference on advances in Neural Information Processing Systems (NIPS), pages 2627\u20132635, 2011. <em>Publisher website HaL<\/em><\/p>\n<p><a href=\"http:\/\/odalricambrymmaillard.wordpress.com\/2014\/08\/03\/finite-sample-analysis-of-bellman-residual-minimization\/\"><strong>Finite sample analysis of bellman residual minimization<\/strong><\/a>, O.-A. Maillard, R. Munos, A. Lazaric, and M. Ghavamzadeh, in Proceedings of the Asian Conference on Machine Learning (ACML), 2010. <em>Publisher website HaL<\/em><\/p>\n<p><a href=\"http:\/\/odalricambrymmaillard.wordpress.com\/2014\/08\/03\/lstd-with-random-projections\/\"><strong>LSTD with random projections<\/strong><\/a>, M. Ghavamzadeh, A. Lazaric, O.-A. Maillard, and R. Munos, In J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R.S. Zemel, and A. Culotta, editors, Proceedings of 23rd conference on advances in Neural Information Processing Systems (NIPS) (NIPS), pages 721\u2013729, 2010. <em>Publisher website HaL<\/em><\/p>\n","protected":false},"excerpt":{"rendered":"<p>\u201cHow hard is my MDP?\u201d Distribution-norm to the rescue, Odalric-Ambrym Maillard, Timothy A. Mann and Shie Mannor in Proceedings of the 27th conference on advances in Neural Information Processing Systems (NIPS), 2014. Publisher website HaL Selecting Near-Optimal Approximate State Representations in Reinforcement Learning, R.Ortner, O.-A. Maillard and D. Ryabko, in\u2026<\/p>\n<p class=\"continue-reading-button\"> <a class=\"continue-reading-link\" href=\"http:\/\/odalricambrymmaillard.neowordpress.fr\/research-areas\/reinforcement-learning\/\">Continue reading<i class=\"crycon-right-dir\"><\/i><\/a><\/p>\n","protected":false},"author":20252,"featured_media":0,"parent":25,"menu_order":0,"comment_status":"closed","ping_status":"closed","template":"","meta":{"_links_to":"","_links_to_target":""},"yoast_head":"<!-- This site is optimized with the Yoast SEO plugin v21.1 - https:\/\/yoast.com\/wordpress\/plugins\/seo\/ -->\n<title>Reinforcement Learning - Odalric-Ambrym Maillard<\/title>\n<meta name=\"robots\" content=\"index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1\" \/>\n<link rel=\"canonical\" href=\"http:\/\/odalricambrymmaillard.neowordpress.fr\/research-areas\/reinforcement-learning\/\" \/>\n<meta property=\"og:locale\" content=\"fr_FR\" \/>\n<meta property=\"og:type\" content=\"article\" \/>\n<meta property=\"og:title\" content=\"Reinforcement Learning - Odalric-Ambrym Maillard\" \/>\n<meta property=\"og:description\" content=\"\u201cHow hard is my MDP?\u201d Distribution-norm to the rescue, Odalric-Ambrym Maillard, Timothy A. Mann and Shie Mannor in Proceedings of the 27th conference on advances in Neural Information Processing Systems (NIPS), 2014. Publisher website HaL Selecting Near-Optimal Approximate State Representations in Reinforcement Learning, R.Ortner, O.-A. Maillard and D. Ryabko, in\u2026 Continue reading\" \/>\n<meta property=\"og:url\" content=\"http:\/\/odalricambrymmaillard.neowordpress.fr\/research-areas\/reinforcement-learning\/\" \/>\n<meta property=\"og:site_name\" content=\"Odalric-Ambrym Maillard\" \/>\n<meta property=\"article:modified_time\" content=\"2016-03-12T11:40:33+00:00\" \/>\n<meta name=\"twitter:card\" content=\"summary_large_image\" \/>\n<meta name=\"twitter:label1\" content=\"Dur\u00e9e de lecture estim\u00e9e\" \/>\n\t<meta name=\"twitter:data1\" content=\"2 minutes\" \/>\n<script type=\"application\/ld+json\" class=\"yoast-schema-graph\">{\"@context\":\"https:\/\/schema.org\",\"@graph\":[{\"@type\":\"WebPage\",\"@id\":\"http:\/\/odalricambrymmaillard.neowordpress.fr\/research-areas\/reinforcement-learning\/\",\"url\":\"http:\/\/odalricambrymmaillard.neowordpress.fr\/research-areas\/reinforcement-learning\/\",\"name\":\"Reinforcement Learning - Odalric-Ambrym Maillard\",\"isPartOf\":{\"@id\":\"http:\/\/odalricambrymmaillard.neowordpress.fr\/#website\"},\"datePublished\":\"2016-03-12T10:49:17+00:00\",\"dateModified\":\"2016-03-12T11:40:33+00:00\",\"breadcrumb\":{\"@id\":\"http:\/\/odalricambrymmaillard.neowordpress.fr\/research-areas\/reinforcement-learning\/#breadcrumb\"},\"inLanguage\":\"fr-FR\",\"potentialAction\":[{\"@type\":\"ReadAction\",\"target\":[\"http:\/\/odalricambrymmaillard.neowordpress.fr\/research-areas\/reinforcement-learning\/\"]}]},{\"@type\":\"BreadcrumbList\",\"@id\":\"http:\/\/odalricambrymmaillard.neowordpress.fr\/research-areas\/reinforcement-learning\/#breadcrumb\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"name\":\"Accueil\",\"item\":\"http:\/\/odalricambrymmaillard.neowordpress.fr\/\"},{\"@type\":\"ListItem\",\"position\":2,\"name\":\"Research Areas\",\"item\":\"http:\/\/odalricambrymmaillard.neowordpress.fr\/research-areas\/\"},{\"@type\":\"ListItem\",\"position\":3,\"name\":\"Reinforcement Learning\"}]},{\"@type\":\"WebSite\",\"@id\":\"http:\/\/odalricambrymmaillard.neowordpress.fr\/#website\",\"url\":\"http:\/\/odalricambrymmaillard.neowordpress.fr\/\",\"name\":\"Odalric-Ambrym Maillard\",\"description\":\"My journey in Mathematics and Machine Learning: towards a better world.\",\"potentialAction\":[{\"@type\":\"SearchAction\",\"target\":{\"@type\":\"EntryPoint\",\"urlTemplate\":\"http:\/\/odalricambrymmaillard.neowordpress.fr\/?s={search_term_string}\"},\"query-input\":\"required name=search_term_string\"}],\"inLanguage\":\"fr-FR\"}]}<\/script>\n<!-- \/ Yoast SEO plugin. -->","yoast_head_json":{"title":"Reinforcement Learning - Odalric-Ambrym Maillard","robots":{"index":"index","follow":"follow","max-snippet":"max-snippet:-1","max-image-preview":"max-image-preview:large","max-video-preview":"max-video-preview:-1"},"canonical":"http:\/\/odalricambrymmaillard.neowordpress.fr\/research-areas\/reinforcement-learning\/","og_locale":"fr_FR","og_type":"article","og_title":"Reinforcement Learning - Odalric-Ambrym Maillard","og_description":"\u201cHow hard is my MDP?\u201d Distribution-norm to the rescue, Odalric-Ambrym Maillard, Timothy A. Mann and Shie Mannor in Proceedings of the 27th conference on advances in Neural Information Processing Systems (NIPS), 2014. Publisher website HaL Selecting Near-Optimal Approximate State Representations in Reinforcement Learning, R.Ortner, O.-A. Maillard and D. Ryabko, in\u2026 Continue reading","og_url":"http:\/\/odalricambrymmaillard.neowordpress.fr\/research-areas\/reinforcement-learning\/","og_site_name":"Odalric-Ambrym Maillard","article_modified_time":"2016-03-12T11:40:33+00:00","twitter_card":"summary_large_image","twitter_misc":{"Dur\u00e9e de lecture estim\u00e9e":"2 minutes"},"schema":{"@context":"https:\/\/schema.org","@graph":[{"@type":"WebPage","@id":"http:\/\/odalricambrymmaillard.neowordpress.fr\/research-areas\/reinforcement-learning\/","url":"http:\/\/odalricambrymmaillard.neowordpress.fr\/research-areas\/reinforcement-learning\/","name":"Reinforcement Learning - Odalric-Ambrym Maillard","isPartOf":{"@id":"http:\/\/odalricambrymmaillard.neowordpress.fr\/#website"},"datePublished":"2016-03-12T10:49:17+00:00","dateModified":"2016-03-12T11:40:33+00:00","breadcrumb":{"@id":"http:\/\/odalricambrymmaillard.neowordpress.fr\/research-areas\/reinforcement-learning\/#breadcrumb"},"inLanguage":"fr-FR","potentialAction":[{"@type":"ReadAction","target":["http:\/\/odalricambrymmaillard.neowordpress.fr\/research-areas\/reinforcement-learning\/"]}]},{"@type":"BreadcrumbList","@id":"http:\/\/odalricambrymmaillard.neowordpress.fr\/research-areas\/reinforcement-learning\/#breadcrumb","itemListElement":[{"@type":"ListItem","position":1,"name":"Accueil","item":"http:\/\/odalricambrymmaillard.neowordpress.fr\/"},{"@type":"ListItem","position":2,"name":"Research Areas","item":"http:\/\/odalricambrymmaillard.neowordpress.fr\/research-areas\/"},{"@type":"ListItem","position":3,"name":"Reinforcement Learning"}]},{"@type":"WebSite","@id":"http:\/\/odalricambrymmaillard.neowordpress.fr\/#website","url":"http:\/\/odalricambrymmaillard.neowordpress.fr\/","name":"Odalric-Ambrym Maillard","description":"My journey in Mathematics and Machine Learning: towards a better world.","potentialAction":[{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"http:\/\/odalricambrymmaillard.neowordpress.fr\/?s={search_term_string}"},"query-input":"required name=search_term_string"}],"inLanguage":"fr-FR"}]}},"_links":{"self":[{"href":"http:\/\/odalricambrymmaillard.neowordpress.fr\/wp-json\/wp\/v2\/pages\/39"}],"collection":[{"href":"http:\/\/odalricambrymmaillard.neowordpress.fr\/wp-json\/wp\/v2\/pages"}],"about":[{"href":"http:\/\/odalricambrymmaillard.neowordpress.fr\/wp-json\/wp\/v2\/types\/page"}],"author":[{"embeddable":true,"href":"http:\/\/odalricambrymmaillard.neowordpress.fr\/wp-json\/wp\/v2\/users\/20252"}],"replies":[{"embeddable":true,"href":"http:\/\/odalricambrymmaillard.neowordpress.fr\/wp-json\/wp\/v2\/comments?post=39"}],"version-history":[{"count":1,"href":"http:\/\/odalricambrymmaillard.neowordpress.fr\/wp-json\/wp\/v2\/pages\/39\/revisions"}],"predecessor-version":[{"id":40,"href":"http:\/\/odalricambrymmaillard.neowordpress.fr\/wp-json\/wp\/v2\/pages\/39\/revisions\/40"}],"up":[{"embeddable":true,"href":"http:\/\/odalricambrymmaillard.neowordpress.fr\/wp-json\/wp\/v2\/pages\/25"}],"wp:attachment":[{"href":"http:\/\/odalricambrymmaillard.neowordpress.fr\/wp-json\/wp\/v2\/media?parent=39"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}