{"version":"1.0","provider_name":"Odalric-Ambrym Maillard","provider_url":"http:\/\/odalricambrymmaillard.neowordpress.fr","title":"Reinforcement Learning - Odalric-Ambrym Maillard","type":"rich","width":600,"height":338,"html":"<blockquote class=\"wp-embedded-content\" data-secret=\"zSmIdmcy6T\"><a href=\"http:\/\/odalricambrymmaillard.neowordpress.fr\/research-areas\/reinforcement-learning\/\">Reinforcement Learning<\/a><\/blockquote><iframe sandbox=\"allow-scripts\" security=\"restricted\" src=\"http:\/\/odalricambrymmaillard.neowordpress.fr\/research-areas\/reinforcement-learning\/embed\/#?secret=zSmIdmcy6T\" width=\"600\" height=\"338\" title=\"\u00ab\u00a0Reinforcement Learning\u00a0\u00bb &#8212; Odalric-Ambrym Maillard\" data-secret=\"zSmIdmcy6T\" frameborder=\"0\" marginwidth=\"0\" marginheight=\"0\" scrolling=\"no\" class=\"wp-embedded-content\"><\/iframe><script type=\"text\/javascript\">\n\/*! This file is auto-generated *\/\n!function(c,l){\"use strict\";var e=!1,o=!1;if(l.querySelector)if(c.addEventListener)e=!0;if(c.wp=c.wp||{},c.wp.receiveEmbedMessage);else if(c.wp.receiveEmbedMessage=function(e){var t=e.data;if(!t);else if(!(t.secret||t.message||t.value));else if(\/[^a-zA-Z0-9]\/.test(t.secret));else{for(var r,s,a,i=l.querySelectorAll('iframe[data-secret=\"'+t.secret+'\"]'),n=l.querySelectorAll('blockquote[data-secret=\"'+t.secret+'\"]'),o=0;o<n.length;o++)n[o].style.display=\"none\";for(o=0;o<i.length;o++)if(r=i[o],e.source!==r.contentWindow);else{if(r.removeAttribute(\"style\"),\"height\"===t.message){if(1e3<(s=parseInt(t.value,10)))s=1e3;else if(~~s<200)s=200;r.height=s}if(\"link\"===t.message)if(s=l.createElement(\"a\"),a=l.createElement(\"a\"),s.href=r.getAttribute(\"src\"),a.href=t.value,a.host===s.host)if(l.activeElement===r)c.top.location.href=t.value}}},e)c.addEventListener(\"message\",c.wp.receiveEmbedMessage,!1),l.addEventListener(\"DOMContentLoaded\",t,!1),c.addEventListener(\"load\",t,!1);function t(){if(o);else{o=!0;for(var e,t,r,s=-1!==navigator.appVersion.indexOf(\"MSIE 10\"),a=!!navigator.userAgent.match(\/Trident.*rv:11\\.\/),i=l.querySelectorAll(\"iframe.wp-embedded-content\"),n=0;n<i.length;n++){if(!(r=(t=i[n]).getAttribute(\"data-secret\")))r=Math.random().toString(36).substr(2,10),t.src+=\"#?secret=\"+r,t.setAttribute(\"data-secret\",r);if(s||a)(e=t.cloneNode(!0)).removeAttribute(\"security\"),t.parentNode.replaceChild(e,t);t.contentWindow.postMessage({message:\"ready\",secret:r},\"*\")}}}}(window,document);\n<\/script>\n","description":"\u201cHow hard is my MDP?\u201d Distribution-norm to the rescue, Odalric-Ambrym Maillard, Timothy A. Mann and Shie Mannor in Proceedings of the 27th conference on advances in Neural Information Processing Systems (NIPS), 2014. Publisher website HaL Selecting Near-Optimal Approximate State Representations in Reinforcement Learning, R.Ortner, O.-A. Maillard and D. Ryabko, in\u2026 Continue reading"}